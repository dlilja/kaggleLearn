{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Machine Learning course\n",
    "\n",
    "This is a notebook for the Kaggle machine learning course available att www.kaggle.com. It uses `scikit-learn` version 0.19.2 and `pandas` version 0.23.3. The Iowa Housing data was downloaded from https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt. \n",
    "\n",
    "We will begin by loading and exploring the data a little bit to see how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns are:\n",
      "Index(['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area',\n",
      "       'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities',\n",
      "       'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n",
      "       'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual',\n",
      "       'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style',\n",
      "       'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type',\n",
      "       'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual',\n",
      "       'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1',\n",
      "       'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF',\n",
      "       'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF',\n",
      "       '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath',\n",
      "       'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr',\n",
      "       'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional',\n",
      "       'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt',\n",
      "       'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual',\n",
      "       'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF',\n",
      "       'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC',\n",
      "       'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type',\n",
      "       'Sale Condition', 'SalePrice'],\n",
      "      dtype='object')\n",
      "Description:\n",
      "            Order           PID  MS SubClass  Lot Frontage       Lot Area  \\\n",
      "count  2930.00000  2.930000e+03  2930.000000   2440.000000    2930.000000   \n",
      "mean   1465.50000  7.144645e+08    57.387372     69.224590   10147.921843   \n",
      "std     845.96247  1.887308e+08    42.638025     23.365335    7880.017759   \n",
      "min       1.00000  5.263011e+08    20.000000     21.000000    1300.000000   \n",
      "25%     733.25000  5.284770e+08    20.000000     58.000000    7440.250000   \n",
      "50%    1465.50000  5.354536e+08    50.000000     68.000000    9436.500000   \n",
      "75%    2197.75000  9.071811e+08    70.000000     80.000000   11555.250000   \n",
      "max    2930.00000  1.007100e+09   190.000000    313.000000  215245.000000   \n",
      "\n",
      "       Overall Qual  Overall Cond   Year Built  Year Remod/Add  Mas Vnr Area  \\\n",
      "count   2930.000000   2930.000000  2930.000000     2930.000000   2907.000000   \n",
      "mean       6.094881      5.563140  1971.356314     1984.266553    101.896801   \n",
      "std        1.411026      1.111537    30.245361       20.860286    179.112611   \n",
      "min        1.000000      1.000000  1872.000000     1950.000000      0.000000   \n",
      "25%        5.000000      5.000000  1954.000000     1965.000000      0.000000   \n",
      "50%        6.000000      5.000000  1973.000000     1993.000000      0.000000   \n",
      "75%        7.000000      6.000000  2001.000000     2004.000000    164.000000   \n",
      "max       10.000000      9.000000  2010.000000     2010.000000   1600.000000   \n",
      "\n",
      "           ...        Wood Deck SF  Open Porch SF  Enclosed Porch  \\\n",
      "count      ...         2930.000000    2930.000000     2930.000000   \n",
      "mean       ...           93.751877      47.533447       23.011604   \n",
      "std        ...          126.361562      67.483400       64.139059   \n",
      "min        ...            0.000000       0.000000        0.000000   \n",
      "25%        ...            0.000000       0.000000        0.000000   \n",
      "50%        ...            0.000000      27.000000        0.000000   \n",
      "75%        ...          168.000000      70.000000        0.000000   \n",
      "max        ...         1424.000000     742.000000     1012.000000   \n",
      "\n",
      "        3Ssn Porch  Screen Porch    Pool Area      Misc Val      Mo Sold  \\\n",
      "count  2930.000000   2930.000000  2930.000000   2930.000000  2930.000000   \n",
      "mean      2.592491     16.002048     2.243345     50.635154     6.216041   \n",
      "std      25.141331     56.087370    35.597181    566.344288     2.714492   \n",
      "min       0.000000      0.000000     0.000000      0.000000     1.000000   \n",
      "25%       0.000000      0.000000     0.000000      0.000000     4.000000   \n",
      "50%       0.000000      0.000000     0.000000      0.000000     6.000000   \n",
      "75%       0.000000      0.000000     0.000000      0.000000     8.000000   \n",
      "max     508.000000    576.000000   800.000000  17000.000000    12.000000   \n",
      "\n",
      "           Yr Sold      SalePrice  \n",
      "count  2930.000000    2930.000000  \n",
      "mean   2007.790444  180796.060068  \n",
      "std       1.316613   79886.692357  \n",
      "min    2006.000000   12789.000000  \n",
      "25%    2007.000000  129500.000000  \n",
      "50%    2008.000000  160000.000000  \n",
      "75%    2009.000000  213500.000000  \n",
      "max    2010.000000  755000.000000  \n",
      "\n",
      "[8 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iowa_data = pd.read_csv(\"./data/AmesHousing.txt\", delimiter = '\\t')\n",
    "\n",
    "print(\"The columns are:\")\n",
    "print(iowa_data.columns)\n",
    "\n",
    "print(\"Description:\")\n",
    "print(iowa_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Order  SalePrice\n",
      "0      1     215000\n",
      "1      2     105000\n",
      "2      3     172000\n",
      "3      4     244000\n",
      "4      5     189900\n"
     ]
    }
   ],
   "source": [
    "# Let's extract all the sales prices of each house.\n",
    "\n",
    "selection = [\"Order\", \"SalePrice\"]\n",
    "sale_prices = iowa_data[selection]\n",
    "print(sale_prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to make a first, albeit silly for reasons we will see later, model. For this one we will use a so called *decision tree*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual prices:\n",
      "0    215000\n",
      "1    105000\n",
      "2    172000\n",
      "3    244000\n",
      "4    189900\n",
      "Name: SalePrice, dtype: int64\n",
      "Predicted prices:\n",
      "[215000. 105000. 172000. 244000. 189900.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "first_model = DecisionTreeRegressor(random_state = 0)\n",
    "y = sale_prices.SalePrice\n",
    "predictors = [\"Lot Area\", \"Year Built\", \"1st Flr SF\", \"2nd Flr SF\", \"Full Bath\", \"Bedroom AbvGr\", \"TotRms AbvGrd\"]\n",
    "X = iowa_data[predictors]\n",
    "\n",
    "first_model.fit(X, y)\n",
    "\n",
    "# With the model trained let's have it \"predict\" some prices.\n",
    "\n",
    "print(\"Actual prices:\")\n",
    "print(y.head())\n",
    "\n",
    "print(\"Predicted prices:\")\n",
    "print(first_model.predict(X.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the accuracy of out first model we use the *mean absolute error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error of our model on our sample is:\n",
      "139.54379977246873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "predicted_prices = first_model.predict(X)\n",
    "error = mae(y, predicted_prices)\n",
    "print(\"The mean absolute error of our model on our sample is:\")\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score doesn't really tell us much however since it was calculated on data that the model was trained on. In order to get a better understanding of the accuracy of our model we will split our data into two sets: a *training set* and a *validation set*. We will then train a new model on the training set and evaluate it using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "train_X, val_X, train_y, val_y = split(X, y, random_state = 0)\n",
    "snd_model = DecisionTreeRegressor()\n",
    "snd_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new model trained let's evaluate it on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual prices:\n",
      "2216    220000\n",
      "836     143000\n",
      "2396    281000\n",
      "1962    135000\n",
      "305     102776\n",
      "Name: SalePrice, dtype: int64\n",
      "Predicted price:\n",
      "[257500. 155000. 233230. 129000.  96500.]\n",
      "Mean absolute error is:\n",
      "27732.587994542973\n"
     ]
    }
   ],
   "source": [
    "# With our new model trained let's evaluate it on the validation set.\n",
    "\n",
    "predictions = snd_model.predict(val_X)\n",
    "\n",
    "print(\"Actual prices:\")\n",
    "print(val_y.head())\n",
    "print(\"Predicted price:\")\n",
    "print(predictions[:5])\n",
    "\n",
    "print(\"Mean absolute error is:\")\n",
    "print(mae(predictions, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basics down of how to train and evaluate a model let's explore some ways of improving a model by tuning hyperparameters, in this case the number of leaf nodes. First we create a helper function to train a model with a specific number of leaf nodes on a training set and return its mean absolute error on a validation set. We will then loop through different values for the maximum number of leaf nodes and pick the best one we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 50 \t\t Mean absolute error: 24989.409446993875\n",
      "Max leaf nodes: 100 \t\t Mean absolute error: 24401.58376237393\n",
      "Max leaf nodes: 250 \t\t Mean absolute error: 24664.182995823816\n",
      "Max leaf nodes: 500 \t\t Mean absolute error: 26177.366842277108\n",
      "Max leaf nodes: 1000 \t\t Mean absolute error: 27688.295652880544\n",
      "Max leaf nodes: 2500 \t\t Mean absolute error: 27549.8690313779\n",
      "Max leaf nodes: 5000 \t\t Mean absolute error: 27549.8690313779\n"
     ]
    }
   ],
   "source": [
    "def get_mae(leaves, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes = leaves, random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(val_X)\n",
    "    return(mae(predictions, val_y))\n",
    "\n",
    "for leaves in [50, 100, 250, 500, 1000, 2500, 5000]:\n",
    "    print(\"Max leaf nodes: {arg1} \\t\\t Mean absolute error: {arg2}\"\n",
    "          .format(arg1 = leaves, arg2 = get_mae(leaves, train_X, val_X, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that 100 maximum leaf nodes seems to be the optimal choice. With the decision tree model explored let's move on to a generalization of it called a *random decision forest*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error of the random forest model is:\n",
      "21554.63075423894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/.local/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state = 0)\n",
    "forest_model.fit(train_X, train_y)\n",
    "predictions = forest_model.predict(val_X)\n",
    "print(\"The mean absolute error of the random forest model is:\")\n",
    "print(mae(predictions, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced topics\n",
    "\n",
    "That covers the basics of creating and evaluating machine learning models. We will now move on to more advanced topics and begin by exploring the problem of handling missing values. First we need to find which columns actually contain some missing data. We will try three progressively more advanced methods: dropping columns with missing values, imputing missing values and finally imputing missing values and marking which fields were missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns are missing data:\n",
      "['Lot Frontage', 'Mas Vnr Area', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath', 'Garage Yr Blt', 'Garage Cars', 'Garage Area']\n"
     ]
    }
   ],
   "source": [
    "# Finding all columns with missing data. We will restrict ourselves to numeric data for convenience.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "numeric_data = iowa_data.select_dtypes(include = np.number)\n",
    "\n",
    "columns_with_missing_data = [col for col in numeric_data.columns if iowa_data[col].isnull().any()]\n",
    "print(\"The following columns are missing data:\")\n",
    "print(columns_with_missing_data)\n",
    "\n",
    "# Extend predictors with some columns missing data and create new training and validation sets.\n",
    "\n",
    "extra_columns = [\"Lot Frontage\", \"Total Bsmt SF\", \"Garage Yr Blt\", \"Garage Area\"]\n",
    "predictors_with_missing = predictors + extra_columns\n",
    "X = numeric_data[predictors_with_missing]\n",
    "y = numeric_data.SalePrice\n",
    "train_X, val_X, train_y, val_y = split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of the reduced mdoel:\n",
      "21554.63075423894\n"
     ]
    }
   ],
   "source": [
    "# Remove columns with missing data from training and validation sets.\n",
    "\n",
    "reduced_train_X = train_X.drop(extra_columns, axis = 1)\n",
    "reduced_val_X = val_X.drop(extra_columns, axis = 1)\n",
    "\n",
    "# Creating and evaluating the model with dropped columns. This is the same as our previous model.\n",
    "\n",
    "reduced_model = RandomForestRegressor(random_state = 0)\n",
    "reduced_model.fit(reduced_train_X, train_y)\n",
    "print(\"Mean absolute error of the reduced mdoel:\")\n",
    "print(mae(reduced_model.predict(reduced_val_X), val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of the imputed model:\n",
      "21399.862495939717\n"
     ]
    }
   ],
   "source": [
    "# Next we will impute values.\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy = \"mean\", axis = 0)\n",
    "\n",
    "impute_train_X = pd.DataFrame(imputer.fit_transform(train_X), index = train_X.index, columns = train_X.columns)\n",
    "impute_val_X = pd.DataFrame(imputer.transform(val_X), index = val_X.index, columns = val_X.columns)\n",
    "impute_model = RandomForestRegressor(random_state = 0)\n",
    "impute_model.fit(impute_train_X, train_y)\n",
    "print(\"Mean absolute error of the imputed model:\")\n",
    "print(mae(impute_model.predict(impute_val_X), val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error for the marked model:\n",
      "21812.31574741766\n"
     ]
    }
   ],
   "source": [
    "# Lastly we impute and mark which fields were imputed.\n",
    "\n",
    "marked_train_X = train_X.copy()\n",
    "marked_val_X = val_X.copy()\n",
    "\n",
    "for col in extra_columns:\n",
    "    marked_train_X[col + \"_was_missing\"] = marked_train_X[col].isnull()\n",
    "    marked_val_X[col + \"_was_missing\"] = marked_val_X[col].isnull()\n",
    "    \n",
    "imputer = Imputer(strategy = \"mean\", axis = 0)\n",
    "marked_train_X = imputer.fit_transform(marked_train_X)\n",
    "marked_val_X = imputer.transform(marked_val_X)\n",
    "\n",
    "marked_model = RandomForestRegressor(random_state = 0)\n",
    "marked_model.fit(marked_train_X, train_y)\n",
    "\n",
    "print(\"Mean absolute error for the marked model:\")\n",
    "print(mae(marked_model.predict(marked_val_X), val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case imputing values did not do much to help our model, perhaps because we chose the wrong predictors. Let's move on with including categorical fields into our model. For now we will consider only a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MS Zoning', 'Street', 'Alley', 'Lot Shape', 'Land Contour',\n",
       "       'Utilities', 'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1',\n",
       "       'Condition 2', 'Bldg Type', 'House Style', 'Roof Style', 'Roof Matl',\n",
       "       'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Exter Qual',\n",
       "       'Exter Cond', 'Foundation', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure',\n",
       "       'BsmtFin Type 1', 'BsmtFin Type 2', 'Heating', 'Heating QC',\n",
       "       'Central Air', 'Electrical', 'Kitchen Qual', 'Functional',\n",
       "       'Fireplace Qu', 'Garage Type', 'Garage Finish', 'Garage Qual',\n",
       "       'Garage Cond', 'Paved Drive', 'Pool QC', 'Fence', 'Misc Feature',\n",
       "       'Sale Type', 'Sale Condition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's select only the categorical data and see its columns.\n",
    "\n",
    "categorical_data = iowa_data.select_dtypes(include = object)\n",
    "categorical_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of the one-hot encoded model:\n",
      "20189.36913532125\n"
     ]
    }
   ],
   "source": [
    "# Let's chose some categorical columns and add them to our predictors.\n",
    "\n",
    "categorical_columns = [\"MS Zoning\", \"Neighborhood\", \"Bldg Type\", \"Pool QC\", \"Sale Type\", \"Sale Condition\"]\n",
    "predictors_with_categories = predictors + categorical_columns\n",
    "X = iowa_data[predictors_with_categories]\n",
    "encoded_X = pd.get_dummies(X)\n",
    "\n",
    "# Let's make new training and validation sets.\n",
    "\n",
    "train_X, val_X, train_y, val_y = split(encoded_X, y, random_state = 0)\n",
    "\n",
    "encoded_model = RandomForestRegressor(random_state = 0)\n",
    "encoded_model.fit(train_X, train_y)\n",
    "\n",
    "print(\"Mean absolute error of the one-hot encoded model:\")\n",
    "print(mae(encoded_model.predict(val_X), val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we'll look into another way of improving our models called *gradient boosting*. For this we will use the `XGBoost` library which includes a gradient boosted decision tree regressor. We will use the same test and validation sets as in the encoding example for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of the gradient boosted decision tree model:\n",
      "19189.39043315143\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "boosted_model = XGBRegressor(silent = True, random_state = 0)\n",
    "boosted_model.fit(train_X, train_y)\n",
    "print(\"Mean absolute error of the gradient boosted decision tree model:\")\n",
    "print(mae(val_y, boosted_model.predict(val_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extra comparison let's also compare it to `scikit-learn`s built in implementation of a gradient boosted decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of the built-in gradient boosted decision tree model:\n",
      "18990.403118871367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "boosted_model_2 = GradientBoostingRegressor(random_state = 0)\n",
    "boosted_model_2.fit(train_X, train_y)\n",
    "print(\"Mean absolute error of the built-in gradient boosted decision tree model:\")\n",
    "print(mae(val_y, boosted_model_2.predict(val_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore some of the ways of tuning hyperparameters for the `XGBRegressor`. First we will try to find an optimal value for `n_estimators` using the `early_stopping_rounds` argument by stopping the iterations if we haven't improved the mean absolute error in `early_stopping_rounds` number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:163353\n",
      "Will train until validation_0-mae hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-mae:147121\n",
      "[2]\tvalidation_0-mae:132471\n",
      "[3]\tvalidation_0-mae:119235\n",
      "[4]\tvalidation_0-mae:107372\n",
      "[5]\tvalidation_0-mae:96811.6\n",
      "[6]\tvalidation_0-mae:87466.9\n",
      "[7]\tvalidation_0-mae:79103.9\n",
      "[8]\tvalidation_0-mae:71581.9\n",
      "[9]\tvalidation_0-mae:64935.2\n",
      "[10]\tvalidation_0-mae:58970.3\n",
      "[11]\tvalidation_0-mae:53687.4\n",
      "[12]\tvalidation_0-mae:49006.6\n",
      "[13]\tvalidation_0-mae:44858.3\n",
      "[14]\tvalidation_0-mae:41263.4\n",
      "[15]\tvalidation_0-mae:38225.3\n",
      "[16]\tvalidation_0-mae:35571.1\n",
      "[17]\tvalidation_0-mae:33324.4\n",
      "[18]\tvalidation_0-mae:31487.1\n",
      "[19]\tvalidation_0-mae:29970.5\n",
      "[20]\tvalidation_0-mae:28645.5\n",
      "[21]\tvalidation_0-mae:27371.6\n",
      "[22]\tvalidation_0-mae:26328.1\n",
      "[23]\tvalidation_0-mae:25502.1\n",
      "[24]\tvalidation_0-mae:24878\n",
      "[25]\tvalidation_0-mae:24281.9\n",
      "[26]\tvalidation_0-mae:23837.8\n",
      "[27]\tvalidation_0-mae:23370.3\n",
      "[28]\tvalidation_0-mae:22901.7\n",
      "[29]\tvalidation_0-mae:22654.6\n",
      "[30]\tvalidation_0-mae:22333.4\n",
      "[31]\tvalidation_0-mae:22019.5\n",
      "[32]\tvalidation_0-mae:21822.3\n",
      "[33]\tvalidation_0-mae:21532\n",
      "[34]\tvalidation_0-mae:21448.9\n",
      "[35]\tvalidation_0-mae:21209.1\n",
      "[36]\tvalidation_0-mae:21086\n",
      "[37]\tvalidation_0-mae:21007.3\n",
      "[38]\tvalidation_0-mae:20864.8\n",
      "[39]\tvalidation_0-mae:20695.6\n",
      "[40]\tvalidation_0-mae:20681.7\n",
      "[41]\tvalidation_0-mae:20587.1\n",
      "[42]\tvalidation_0-mae:20533.2\n",
      "[43]\tvalidation_0-mae:20413\n",
      "[44]\tvalidation_0-mae:20425.6\n",
      "[45]\tvalidation_0-mae:20338.2\n",
      "[46]\tvalidation_0-mae:20287.1\n",
      "[47]\tvalidation_0-mae:20237.1\n",
      "[48]\tvalidation_0-mae:20171.5\n",
      "[49]\tvalidation_0-mae:20080.9\n",
      "[50]\tvalidation_0-mae:20020.1\n",
      "[51]\tvalidation_0-mae:19990.9\n",
      "[52]\tvalidation_0-mae:19964.7\n",
      "[53]\tvalidation_0-mae:19959.9\n",
      "[54]\tvalidation_0-mae:19897.7\n",
      "[55]\tvalidation_0-mae:19860.3\n",
      "[56]\tvalidation_0-mae:19860.1\n",
      "[57]\tvalidation_0-mae:19869.8\n",
      "[58]\tvalidation_0-mae:19860.2\n",
      "[59]\tvalidation_0-mae:19804.5\n",
      "[60]\tvalidation_0-mae:19765.9\n",
      "[61]\tvalidation_0-mae:19757.4\n",
      "[62]\tvalidation_0-mae:19748.4\n",
      "[63]\tvalidation_0-mae:19704.7\n",
      "[64]\tvalidation_0-mae:19675.3\n",
      "[65]\tvalidation_0-mae:19631.6\n",
      "[66]\tvalidation_0-mae:19630.6\n",
      "[67]\tvalidation_0-mae:19629.8\n",
      "[68]\tvalidation_0-mae:19608\n",
      "[69]\tvalidation_0-mae:19589.1\n",
      "[70]\tvalidation_0-mae:19575.8\n",
      "[71]\tvalidation_0-mae:19568.2\n",
      "[72]\tvalidation_0-mae:19575.5\n",
      "[73]\tvalidation_0-mae:19576.1\n",
      "[74]\tvalidation_0-mae:19531\n",
      "[75]\tvalidation_0-mae:19526.4\n",
      "[76]\tvalidation_0-mae:19537.9\n",
      "[77]\tvalidation_0-mae:19503.6\n",
      "[78]\tvalidation_0-mae:19483.6\n",
      "[79]\tvalidation_0-mae:19474.1\n",
      "[80]\tvalidation_0-mae:19461.7\n",
      "[81]\tvalidation_0-mae:19433.5\n",
      "[82]\tvalidation_0-mae:19433.5\n",
      "[83]\tvalidation_0-mae:19385\n",
      "[84]\tvalidation_0-mae:19361.8\n",
      "[85]\tvalidation_0-mae:19335.6\n",
      "[86]\tvalidation_0-mae:19342.4\n",
      "[87]\tvalidation_0-mae:19333.7\n",
      "[88]\tvalidation_0-mae:19327.3\n",
      "[89]\tvalidation_0-mae:19339.9\n",
      "[90]\tvalidation_0-mae:19320.8\n",
      "[91]\tvalidation_0-mae:19290.6\n",
      "[92]\tvalidation_0-mae:19285\n",
      "[93]\tvalidation_0-mae:19285.1\n",
      "[94]\tvalidation_0-mae:19273.9\n",
      "[95]\tvalidation_0-mae:19267.2\n",
      "[96]\tvalidation_0-mae:19254.5\n",
      "[97]\tvalidation_0-mae:19260.2\n",
      "[98]\tvalidation_0-mae:19189.3\n",
      "[99]\tvalidation_0-mae:19189.4\n",
      "[100]\tvalidation_0-mae:19171.6\n",
      "[101]\tvalidation_0-mae:19147.7\n",
      "[102]\tvalidation_0-mae:19147.1\n",
      "[103]\tvalidation_0-mae:19150\n",
      "[104]\tvalidation_0-mae:19143\n",
      "[105]\tvalidation_0-mae:19135.9\n",
      "[106]\tvalidation_0-mae:19139.9\n",
      "[107]\tvalidation_0-mae:19139.2\n",
      "[108]\tvalidation_0-mae:19133.7\n",
      "[109]\tvalidation_0-mae:19113.6\n",
      "[110]\tvalidation_0-mae:19091.1\n",
      "[111]\tvalidation_0-mae:19042.5\n",
      "[112]\tvalidation_0-mae:19055.2\n",
      "[113]\tvalidation_0-mae:19055\n",
      "[114]\tvalidation_0-mae:19055.3\n",
      "[115]\tvalidation_0-mae:19049.9\n",
      "[116]\tvalidation_0-mae:19054.5\n",
      "Stopping. Best iteration:\n",
      "[111]\tvalidation_0-mae:19042.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_boosted_model = XGBRegressor(n_estimators = 2000, random_state = 0)\n",
    "tuned_boosted_model.fit(train_X, train_y, early_stopping_rounds = 5,\n",
    "                       eval_set = [(val_X, val_y)], eval_metric = \"mae\")\n",
    "tuned_boosted_model.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that 111 iterations give us the best model. We will continue trying to tune this model a little further, this time by adjusting the `learning_rate` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n iterations: 901\tLearning rate: 0.01\tMAE: 19341.33800520123\n",
      "n iterations: 406\tLearning rate: 0.020000000000000004\tMAE: 19535.59997442019\n",
      "n iterations: 229\tLearning rate: 0.030000000000000006\tMAE: 19701.711854109824\n",
      "n iterations: 222\tLearning rate: 0.04000000000000001\tMAE: 19414.564269270122\n",
      "n iterations: 149\tLearning rate: 0.05000000000000001\tMAE: 19579.37616174966\n",
      "n iterations: 208\tLearning rate: 0.06000000000000001\tMAE: 19036.778398917122\n",
      "n iterations: 167\tLearning rate: 0.07\tMAE: 19052.983650238744\n",
      "n iterations: 79\tLearning rate: 0.08\tMAE: 19706.30301415416\n",
      "n iterations: 66\tLearning rate: 0.09000000000000001\tMAE: 19640.327624062073\n",
      "n iterations: 111\tLearning rate: 0.1\tMAE: 19054.502057042973\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for rate in np.linspace(0.01, 0.1, 10):\n",
    "    model = XGBRegressor(n_estimators = 2000, learning_rate = rate, random_state = 0)\n",
    "    model.fit(train_X, train_y, early_stopping_rounds = 5,\n",
    "             eval_set = [(val_X, val_y)], eval_metric = \"mae\",\n",
    "             verbose = False)\n",
    "    print(\"n iterations: {arg1}\\tLearning rate: {arg2}\\tMAE: {arg3}\"\n",
    "         .format(arg1 = model.best_iteration, arg2 = rate, arg3 = mae(val_y, model.predict(val_X))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we get the best model using `n_estimators =  208` and `learning_rate = 0.06`, at least by only comparing the mean absolute error. However none turned out better than `scikit-learn`'s built in version, hence we will use this one, though we note that `xgboost` has other advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pretty good model let's try to understand what this model is actually telling us by looking at some *partial dependence plots*. Let's pick a couple predictors and see what our model tells us about how the price depends on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import partial_dependence as pdep, plot_partial_dependence as plot_pdep\n",
    "\n",
    "boosted_model = GradientBoostingRegressor(random_state = 0)\n",
    "boosted_model.fit(train_X, train_y)\n",
    "\n",
    "plots = plot_pdep(boosted_model, \n",
    "                  features = [0, 1, 5], \n",
    "                  X = train_X, \n",
    "                  feature_names = list(train_X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots it seems, for example, that our model predicts higher prices the more modern the house is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to put all of this together into what is called a *pipeline*. Pipelines string several steps together which helps us keep our code clean and, hopefully, bug free. To really illustrate the power of pipelines we will go through the entire process again: loading the data, creating training and validation sets, taking care of categorical and missing data, training and evaluating the model. This time let's also include all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15767.03946492763"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Loading the data\n",
    "iowa_data = pd.read_csv(\"./data/AmesHousing.txt\", delimiter = '\\t')\n",
    "y = iowa_data.SalePrice\n",
    "X = iowa_data.drop([\"Order\", \"SalePrice\"], axis = 1)\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "categorical = X.select_dtypes(include = \"object\")\n",
    "X = X.drop(categorical.columns, axis = 1).join(pd.get_dummies(categorical))\n",
    "\n",
    "# Splitting the data\n",
    "train_X, val_X, train_y, val_y = split(X, y, random_state = 0)\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = make_pipeline(Imputer(strategy = \"mean\", axis = 0), GradientBoostingRegressor(random_state = 0))\n",
    "\n",
    "# Fitting the model\n",
    "pipe.fit(train_X, train_y)\n",
    "\n",
    "# Evaluating the model\n",
    "mae(val_y, pipe.predict(val_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we do some cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 15416.831349617949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, scoring = \"neg_mean_absolute_error\")\n",
    "print(\"Mean absolute error: {arg1}\".format(arg1 = -1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making machine learning models one should take care to avoid *data leakage*. This mainly occurs through *leaky predictors* or *leaky validation strategies*. To avoid such issues make sure that you have the correct predictors, that they are all available when the time comes to make a decision and that you only fit transformers and models on training sets. You should always keep an eye out for a model that seems unreasonably accurate on the validation set. It may be due to data leakage and it's predictions may be much more inaccurate on new data. In our case we can see, for example, the predictors `Mo Sold` and `Yr Sold`. If our goal is to predict the sale price of a particular house then this data will not be available to us since the house hasn't been sold yet. It therefore makes sense to remove it from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 15349.564622068012\n"
     ]
    }
   ],
   "source": [
    "X = X.drop([\"Mo Sold\", \"Yr Sold\"], axis = 1)\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, scoring = \"neg_mean_absolute_error\")\n",
    "print(\"Mean absolute error: {arg1}\".format(arg1 = -1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact our model seems to have become slightly better by dropping these columns. It seems that these predictors were not causing data leakage-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
